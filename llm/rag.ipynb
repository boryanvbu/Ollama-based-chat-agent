{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        pdf_document = fitz.open(file_path)\n",
    "        return pdf_document\n",
    "    except Exception as e:\n",
    "        print(\"Error loading PDF:\", e)\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"VSCodeProjects/llm/ml_book.pdf\"\n",
    "pdf_doc = load_pdf(file_path)\n",
    "if pdf_doc:\n",
    "    print(\"PDF loaded successfully!\")\n",
    "    # You can now perform operations on the pdf_doc object\n",
    "else:\n",
    "    print(\"Failed to load PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Responsible for splitting the documents into several chunks\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "    \n",
    "    # Initializing the RecursiveCharacterTextSplitter with\n",
    "    # chunk_size and chunk_overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    # Splitting the documents into chunks\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "    # returning the document chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading the embedding model\n",
    "def load_embedding_model(model_path, normalize_embedding=True):\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs={'device':'cpu'}, # here we will run the model with CPU only\n",
    "        encode_kwargs = {\n",
    "            'normalize_embeddings': normalize_embedding # keep True to compute cosine similarity\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Function for creating embeddings using FAISS\n",
    "def create_embeddings(chunks, embedding_model, storing_path=\"vectorstore\"):\n",
    "    # Creating the embeddings using FAISS\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "    \n",
    "    # Saving the model in current directory\n",
    "    vectorstore.save_local(storing_path)\n",
    "    \n",
    "    # returning the vectorstore\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### System:\n",
    "You are an AI Assistant that follows instructions extreamly well. \\\n",
    "Help as much as you can.\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### System:\n",
    "You are an respectful and honest assistant. You have to answer the user's \\\n",
    "questions using only the context provided to you. If you don't know the answer, \\\n",
    "just say you don't know. Don't try to make up an answer.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### User:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the chain for Question Answering\n",
    "def load_qa_chain(retriever, llm, prompt):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever, # here we are using the vectorstore as a retriever\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True, # including source documents in output\n",
    "        chain_type_kwargs={'prompt': prompt} # customizing the prompt\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettifying the response\n",
    "def get_response(query, chain):\n",
    "    # Getting response from chain\n",
    "    response = chain({'query': query})\n",
    "    \n",
    "    # Wrapping the text for better output in Jupyter Notebook\n",
    "    wrapped_text = textwrap.fill(response['result'], width=100)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang_funcs import *\n",
    "from langchain.llms import Ollama\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading orca-mini from Ollama\n",
    "llm = Ollama(model=\"orca-mini\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading orca-mini from Ollama\n",
    "llm = Ollama(model=\"orca-mini\", temperature=0)\n",
    "\n",
    "# Loading the Embedding Model\n",
    "embed = load_embedding_model(model_path=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and splitting the documents\n",
    "docs = load_pdf_data(file_path=\"VSCodeProjects/llm/ml_book.pdf\")\n",
    "documents = split_docs(documents=docs)\n",
    "\n",
    "# creating vectorstore\n",
    "vectorstore = create_embeddings(documents, embed)\n",
    "\n",
    "# converting vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the prompt from the template which we created before\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Creating the chain\n",
    "chain = load_qa_chain(retriever, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MyLanguageModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming chain represents some language model or conversational context\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Create or obtain the necessary chain object\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mMyLanguageModel\u001b[49m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Call the get_response function with both the input text and the chain argument\u001b[39;00m\n\u001b[1;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m get_response(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is a rain forest?\u001b[39m\u001b[38;5;124m\"\u001b[39m, chain)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MyLanguageModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming chain represents some language model or conversational context\n",
    "# Create or obtain the necessary chain object\n",
    "chain = MyLanguageModel()\n",
    "\n",
    "# Call the get_response function with both the input text and the chain argument\n",
    "response = get_response(\"What is a rain forest?\", chain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
