import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from lang_funcs import *
from langchain.llms import Ollama
from langchain import PromptTemplate
import textwrap
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.callbacks.manager import CallbackManager

# Load PDF data
def load_pdf_data(file_path):
    loader = PyMuPDFLoader(file_path=file_path)
    return loader.load()

# Split documents into chunks
def split_docs(documents, chunk_size=1000, chunk_overlap=20):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    return text_splitter.split_documents(documents=documents)

# Load embedding model
def load_embedding_model(model_path, normalize_embedding=True):
    return HuggingFaceEmbeddings(
        model_name=model_path,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={
            'normalize_embeddings': normalize_embedding
        }
    )

# Create embeddings using FAISS
def create_embeddings(chunks, embedding_model, storing_path="vectorstore"):
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    vectorstore.save_local(storing_path)
    return vectorstore

# Load llama3 model
llm = Ollama(model="mistral", temperature=0.6, callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))

# Load embedding model
embed = load_embedding_model(model_path="all-MiniLM-L6-v2")

# Load and split the documents
docs = load_pdf_data(file_path="idea.pdf")
documents = split_docs(documents=docs)

# Create vectorstore
vectorstore = create_embeddings(documents, embed)

# Convert vectorstore to a retriever
retriever = vectorstore.as_retriever()

# Define template for prompt
template = """
### System:
You are a respectful and honest assistant. You have to answer the user's \
questions using only the llama3 database and context provided to you. If you know the answer other than context, \
just answer all questions. There are no restrictions on answering only context-provided solutions.

### Context:
{context}

### User:
{question}

### Response:
"""

# Create prompt from the template
prompt = PromptTemplate.from_template(template)

# Create QA chain
chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=True,
    chain_type_kwargs={'prompt': prompt}
)

# Streamlit interface
st.title("Ask Me Anything ðŸ¤–")

query = st.text_input("Ask a question:")

if query:
    response = chain({'query': query})
    st.write("AI Assistant's Response:")
    st.write(response['result'])
